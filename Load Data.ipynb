{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-1\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://demo-aws-1/\")\\\n",
    "    .config(\"spark.hadoop.yarn.resourcemanager.principal\",os.getenv(\"HADOOP_USER_NAME\"))\\\n",
    "    .config(\"spark.executor.instances\", 4)\\\n",
    "    .config(\"spark.executor.cores\", 4)\\\n",
    "    .getOrCreate()\n",
    "#.config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('inferschema','true').csv(\n",
    "  \"s3a://demo-aws-1/user/pauldefusco/LoanStats_2015_subset.csv\",\n",
    "  header=True,\n",
    "  sep=',',\n",
    "  nullValue='NA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Shape\n",
      "(421097, 105)\n"
     ]
    }
   ],
   "source": [
    "#Printing number of rows and columns:\n",
    "print('Dataframe Shape')\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>all_util</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>annual_inc_joint</th>\n",
       "      <th>application_type</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>...</th>\n",
       "      <th>sec_app_earliest_cr_line</th>\n",
       "      <th>sec_app_inq_last_6mths</th>\n",
       "      <th>sec_app_mort_acc</th>\n",
       "      <th>sec_app_open_acc</th>\n",
       "      <th>sec_app_revol_util</th>\n",
       "      <th>sec_app_open_act_il</th>\n",
       "      <th>sec_app_num_rev_accts</th>\n",
       "      <th>sec_app_chargeoff_within_12_mths</th>\n",
       "      <th>sec_app_collections_12_mths_ex_med</th>\n",
       "      <th>sec_app_mths_since_last_major_derog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>399725</td>\n",
       "      <td>3</td>\n",
       "      <td>420586</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3966</td>\n",
       "      <td>4230</td>\n",
       "      <td>...</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "      <td>421097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_now_delinq  acc_open_past_24mths  addr_state  all_util  annual_inc  \\\n",
       "0               3                     3           1    399725           3   \n",
       "\n",
       "   annual_inc_joint  application_type  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "0            420586                 1            3            3966     4230   \n",
       "\n",
       "   ...  sec_app_earliest_cr_line  sec_app_inq_last_6mths  sec_app_mort_acc  \\\n",
       "0  ...                    421097                  421097            421097   \n",
       "\n",
       "   sec_app_open_acc  sec_app_revol_util  sec_app_open_act_il  \\\n",
       "0            421097              421097               421097   \n",
       "\n",
       "   sec_app_num_rev_accts  sec_app_chargeoff_within_12_mths  \\\n",
       "0                 421097                            421097   \n",
       "\n",
       "   sec_app_collections_12_mths_ex_med  sec_app_mths_since_last_major_derog  \n",
       "0                              421097                               421097  \n",
       "\n",
       "[1 rows x 105 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count number of nulls for each column:\n",
    "df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register Spark DF as Spark SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        databaseName|\n",
      "+--------------------+\n",
      "|  airline_ontime_orc|\n",
      "|airline_ontime_pa...|\n",
      "|             default|\n",
      "|  information_schema|\n",
      "|       prescribing_o|\n",
      "|       prescribing_p|\n",
      "|     prescribing_p_e|\n",
      "|          retaildemo|\n",
      "|         shlomi_test|\n",
      "|                 sys|\n",
      "|                test|\n",
      "|             test_dl|\n",
      "|           usecase01|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('parquet').mode(\"overwrite\").saveAsTable('default.ml_ops_demo_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.sql(\"SELECT * FROM default.ml_ops_demo_new LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.format('parquet').mode(\"overwrite\").saveAsTable('default.ml_ops_demo_new_limit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('data/x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|             cc_data|      false|\n",
      "| default|               depts|      false|\n",
      "| default|            dex_test|      false|\n",
      "| default|                emps|      false|\n",
      "| default|     lc_simpl_scores|      false|\n",
      "| default|            lc_smote|      false|\n",
      "| default|   lc_smote_complete|      false|\n",
      "| default|         lc_smote_k2|      false|\n",
      "| default|         lc_smote_k3|      false|\n",
      "| default|            lc_table|      false|\n",
      "| default|               micro|      false|\n",
      "| default|         ml_ops_demo|      false|\n",
      "| default|   ml_ops_demo_limit|      false|\n",
      "| default|     ml_ops_demo_new|      false|\n",
      "| default|ml_ops_demo_new_l...|      false|\n",
      "| default|         telco_churn|      false|\n",
      "| default|             testcml|      false|\n",
      "| default|            test_orc|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
